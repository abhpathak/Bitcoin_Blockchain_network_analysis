{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1><center>Scala Graphx to obtain network related features</center></h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import Modules for using sqlcontext, different graph features and other spark functionalities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import org.apache.spark.graphx.Graph\n",
    "import org.apache.spark.sql.SQLContext\n",
    "import org.apache.spark.sql.Row\n",
    "import org.apache.spark.sql.SparkSession\n",
    "import org.apache.spark.rdd.RDD\n",
    "import org.apache.spark.graphx.Graph\n",
    "import org.apache.spark.graphx.Edge\n",
    "import org.apache.spark.graphx.VertexId\n",
    "import org.apache.spark._\n",
    "import org.apache.spark.graphx._\n",
    "import org.apache.spark.rdd.RDD\n",
    "import org.apache.spark.graphx.lib._\n",
    "import org.apache.spark.graphx.lib.PageRank"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### sqlcontext [Entry point for working with structured data in Spark. Allows the creation of DataFrame as well as the execution of SQL queries]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "val sqlContext = new org.apache.spark.sql.SQLContext(sc)\n",
    "import sqlContext.implicits._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load data from s3 | sc.textFile loads each line as a string into RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "val data = sc.textFile(\"s3://qubole-s3bucket-20170108/quobole_default_data_location/txn_pubkey_mapping_2011_2013/part*\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data is converted to tabular (rows and columns) format for further analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Split each line with a comma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "val data_split = data.map(line => line.split(','))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Define column names and datatype for Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "case class X(BLOCK_ID:String,BLOCK_TIME:String,TX_ID:String,SENDER_PUBKEY_ID:String,RECEIVER_PUBKEY_ID:String,TX_IN_POS:String,\n",
    "TX_OUT_POS:String,TX_IN_VALUE:String,TX_OUT_VALUE:String)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Create a dataframe from splitted data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "val df = data_split.map{case Array(s0, s1, s2, s3, s4, s5, s6, s7, s8) => X(s0, s1, s2, s3, s4, s5, s6, s7, s8) }.toDF()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Register it as temporary table so that it can be queried using SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df.registerTempTable(\"txn_pubkey_mapping_2011_2013\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Take sender and receiver id from temp table between required time frame using sqlcontext (defined earlier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "val sept_h1= sqlContext.sql(\"select SENDER_PUBKEY_ID, RECEIVER_PUBKEY_ID from txn_pubkey_mapping_2011_2013 where BLOCK_TIME between '2013-11-01 00:00:00' AND '2013-12-01 00:00:00' \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- SENDER_PUBKEY_ID: string (nullable = true)\n",
      " |-- RECEIVER_PUBKEY_ID: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sept_h1.printSchema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Take sample (or work on entire data) and cache it, otherwise all the steps above will run again and again because of lazy evaluation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MapPartitionsRDD[138] at rdd at <console>:72"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val sampledataRDD = sept_h1.rdd\n",
    "sampledataRDD.cache() //Cache for reuse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Function to convert datatype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def toLong(s: String):Option[Long] = {\n",
    "    try {\n",
    "        Some(s.toLong)\n",
    "    } catch {\n",
    "        case e: NumberFormatException => None\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### To Create a graph object, we need to define vertex and edge class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Create vertex RDD. Doing a union of all the sender and receiver ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "val d1 = sampledataRDD.map(field => field(0))\n",
    "val d2 = sampledataRDD.map(field => field(1))\n",
    "val d3 = d1.union(d2)\n",
    "val d4 = d3.distinct()\n",
    "val d5 = d4.map(row => row+\"\")s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Graph accepts vertex RDD in a format where first element is VertexId type in long format, followed by string datatypes which contains property of a vertex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "val vertex: RDD[(VertexId, (String, String))] =  d5.map(row => (toLong(row).getOrElse(0), ((\"ran\", \"dom\"))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create edge RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "val edgeArray = sampledataRDD.map(row => (row(0)+\"\", row(1)+\"\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Graph accepts edge RDD from Edge class (module imported) where first el ement is \"from id\", second element is \"to id\"  in long format and third element is a string containing the property of edge "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "val edges: RDD[Edge[String]] =  edgeArray.map(row => Edge(toLong(row._1).getOrElse(0), toLong(row._2).getOrElse(0), \"1\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create Graph from vertex and edge RDD created above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "org.apache.spark.graphx.impl.GraphImpl@5a724ab2"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val graphtest = Graph(vertex, edges)\n",
    "graphtest.cache() //cache graph for faster access"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate features from Graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Indegree (In a directed graph, indgree is the number of incoming edges to a vertex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "val indeg = graphtest.inDegrees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Outdegree (In a directed graph, outdgree is the number of outgoing edges from a vertex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "val outdeg = graphtest.outDegrees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PageRank (It is centrality measure of a vertex. It is special case of eigenvector centrality and it reflects the importance of a vertex in the Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "val prGraph = graphtest.pageRank(10).vertices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Triangle Count OR clustering coefficient (It reflects how clustered is the vertex. Higher the immediate neighbors of vertex transact with each other, higher is the value of clustering coefficient)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "val triCounts = graphtest.triangleCount().vertices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "##### Features are saved into S3 bucket so that it can be used later for training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "indeg.saveAsTextFile(\"s3://mcknightelp/notebooks/temp_results/graphx_indeg_nov1_nov30/\")\n",
    "outdeg.saveAsTextFile(\"s3://mcknightelp/notebooks/temp_results/graphx_outdeg_nov1_nov30/\")\n",
    "prGraph.saveAsTextFile(\"s3://mcknightelp/notebooks/temp_results/graphx_pageRank_nov1_nov30/\")\n",
    "triCounts.saveAsTextFile(\"s3://mcknightelp/notebooks/temp_results/graphx_clusterCoef_nov1_nov30/\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Apache Toree - Scala",
   "language": "scala",
   "name": "apache_toree_scala"
  },
  "language_info": {
   "file_extension": ".scala",
   "name": "scala",
   "version": "2.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
